# üß† Friday Local (macOS Edition)

> Um assistente virtual inteligente, privado e offline, projetado para rodar nativamente em Apple Silicon (M1/M2/M3/M4).

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Ollama](https://img.shields.io/badge/AI-Ollama-orange)
![Whisper](https://img.shields.io/badge/ASR-Whisper-green)
![Status](https://img.shields.io/badge/Status-Functional-brightgreen)

## üìñ Sobre o Projeto

O **Friday Local** √© uma implementa√ß√£o moderna de assistente de voz que prioriza privacidade e performance. Diferente de solu√ß√µes como Alexa ou Google Assistant, todo o processamento ‚Äî desde o reconhecimento de fala at√© a gera√ß√£o de resposta ‚Äî acontece **localmente** no seu Mac, aproveitando a Neural Engine dos chips Apple Silicon.

### ‚ú® Funcionalidades Principais
* **üó£Ô∏è Intera√ß√£o H√≠brida:** Suporta comandos por voz (Hands-free) ou texto.
* **üîí Privacidade Total:** Nenhum √°udio ou dado √© enviado para a nuvem.
* **üß† IA Generativa Local:** Utiliza o **Llama 3** via Ollama para processamento de linguagem natural.
* **üëÇ Transcri√ß√£o de Alta Fidelidade:** Integra√ß√£o com **OpenAI Whisper** para entender fala mesmo em ambientes ruidosos.
* **‚ö° Controle do SO:** Capaz de abrir aplicativos e executar comandos do sistema nativamente.
* **üó£Ô∏è Text-to-Speech:** Responde vocalmente utilizando o motor de s√≠ntese nativo do macOS.

---

## üõ†Ô∏è Tecnologias Utilizadas

* **Python 3.x**: Linguagem base.
* **Ollama**: Orquestrador de LLMs locais (rodando Llama 3).
* **OpenAI Whisper**: Modelo SOTA (State of the Art) para Automatic Speech Recognition (ASR).
* **SoundDevice & SciPy**: Captura e processamento de √°udio em tempo real.
* **Subprocess**: Automa√ß√£o e execu√ß√£o de comandos shell.

---

## üöÄ Como Rodar o Projeto

### Pr√©-requisitos
* macOS (Recomendado Apple Silicon M1/M2/M3/M4 para melhor performance).
* Python 3 instalado.
* [Ollama](https://ollama.com) instalado e rodando.
* **FFmpeg** instalado (essencial para o Whisper).

### Instala√ß√£o

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone [https://github.com/JoaoPaulo121212/ia-local.git](https://github.com/JoaoPaulo121212/ia-local.git)
    cd ia-local
    ```

2.  **Prepare o ambiente:**
    ```bash
    # Instale o FFmpeg (via Homebrew)
    brew install ffmpeg

    # Baixe o modelo Llama 3 no Ollama
    ollama run llama3
    ```

3.  **Configure o Python:**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    pip install ollama openai-whisper sounddevice scipy numpy
    ```

4.  **Execute:**
    ```bash
    python3 main.py
    ```

---

## üéÆ Exemplos de Uso

| Comando de Voz | A√ß√£o do Jarvis |
| :--- | :--- |
| *"Open Spotify"* | Abre o aplicativo Spotify nativamente. |
| *"Start Visual Studio Code"* | Inicia o seu editor de c√≥digo. |
| *"What is the capital of France?"* | Responde: *"Paris is the capital of France"* (falado e escrito). |
| *[Enter sem falar]* | Ativa o modo de escuta para comandos r√°pidos. |

---

## üß† Como Funciona (Pipeline)

1.  **Input:** O usu√°rio fala no microfone ou digita um comando.
2.  **Processamento de √Åudio:** Se for voz, o `sounddevice` grava e normaliza o √°udio.
3.  **Transcri√ß√£o:** O `Whisper` converte o √°udio (.wav) em texto (String).
4.  **Inten√ß√£o (Intent Recognition):** O script verifica se √© um comando de sistema (ex: "Open X").
    * *Se for app:* A IA extrai o nome do app e o `subprocess` executa `open -a "App Name"`.
5.  **Racioc√≠nio:** Se n√£o for comando, o texto vai para o `Ollama (Llama 3)` que gera uma resposta curta.
6.  **Output:** A resposta √© lida em voz alta pelo `say` do macOS.

---

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT - sinta-se livre para usar e modificar.
